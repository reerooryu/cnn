# -*- coding: utf-8 -*-
"""Copy of Shrewsbury AM (AI in Healthcare).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Px8OrKjCoYEkOT2vDgelXoK8zq0L5UNx

# AI in Healthcare with HAM10000 for Skin Cancer Detection
"""

#@title Download the Data
import os

# Prepare data
images_1 = os.makedirs('images_1', exist_ok=True)
images_2= os.makedirs('images_2', exist_ok=True)
images_all= os.makedirs('images_all', exist_ok=True)

wget -q --show-progress 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20(Healthcare%20B)%20Skin%20Cancer%20Diagnosis/metadata.csv'
!wget -q --show-progress 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20(Healthcare%20B)%20Skin%20Cancer%20Diagnosis/images_1.zip'
!wget -q --show-progress 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20(Healthcare%20B)%20Skin%20Cancer%20Diagnosis/images_2.zip'
!wget -q --show-progress 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20(Healthcare%20B)%20Skin%20Cancer%20Diagnosis/hmnist_8_8_RGB.csv'
!unzip -q -o images_1.zip -d images_1
!unzip -q -o images_2.zip -d images_2

import os.path
from os import path
from distutils.dir_util import copy_tree

copy_tree('images_1', 'images_all', verbose=1)
copy_tree('images_2', 'images_all', verbose=1)
print("Downloaded Data")

#@title Import Libraries
import pandas as pd
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
import random
import cv2
from collections import defaultdict
from google.colab.patches import cv2_imshow
from PIL import Image
from scipy import stats
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
import tensorflow as tf

"""# Data Validation

The data that we downloaded contains three things:
- A metadata file that has information on the patients
- A folder of skin lesion images (part 1)
- A folder of skin lesion images (part 2)

From the [data repository](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/DBW86T), we know that we should have 10015 images in total (between the two folders) and 10015 lines in the metadata file.
"""

# check if the metadata file has 10015 rows
metadata_path = "/content/metadata.csv"
with open(metadata_path, "r") as f:
    lines = f.readlines()
    print(f"The columns in my metadata file are:")
    for c in lines[0].split(","):
        print(f"    - {c}")
    print(f"The number of rows found was {len(lines[1:])}")

# check if the number of images is 10015
image_data_folder_1 = "/content/images_1"
image_data_folder_2 = "/content/images_2"
print(f"Total number of images across my folders is: {len(os.listdir(image_data_folder_1)) + len(os.listdir(image_data_folder_2))}")



"""# Explore the Data

In exploring the data, we want to:
- Visualize some of the images
- Understand the patterns / distribution from the metadata

## Visualize a Single Image
"""

sample_image_path = "/content/images_1/ISIC_0024316.jpg"
sample_image = np.array(Image.open(sample_image_path))

# make a figure
fig, ax = plt.subplots()
plt.imshow(sample_image)

# Hide the axes
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
ax.spines['bottom'].set_visible(False)
ax.spines['left'].set_visible(False)

# Remove the ticks
ax.set_xticks([])
ax.set_yticks([])

plt.show()

"""## Understand the Metadata"""

# read in our data using pandas
metadata_df = pd.read_csv("/content/metadata.csv")
metadata_df.head()

"""Key Questions:
- How many unique patients are represented in this dataset?
- What is the sex distribution? Both over the images and over unique patients?
- What is the gender distribution? Both over the images and over unique patients?
- Is there a difference between genders when it comes to age (over the patients)?
- What is the location distribution (where is the skin lesion located)? Both over the images and over unique patients?
- What is the distribution of `dx_type`?
- What is the distribution of the diagnosis?

###How many unique patients are represented in this dataset?
"""

print(f"Number of unique patients: {len(metadata_df['lesion_id'].unique())}")

"""### What is the age distribution? Both over the images and over unique patients?"""

# age distribution over the images
age_over_images = metadata_df['age'].dropna()
fig, ax = plt.subplots()
ax.hist(age_over_images)
ax.set_title("Age Distribution over the Images")
ax.set_ylabel("Frequency / Count")
ax.set_xlabel("Age")
plt.tight_layout()
plt.show()

# print statistics about out dataset
print(f"Number of N/A rows for age: {len(metadata_df[metadata_df['age'].isna()])}")
print("Summary Statistics (ignore N/A values):")
print(f"    - {'Count':<10}: {len(age_over_images)}")
print(f"    - {'Mean':<10}: {np.mean(age_over_images):.2f}")
print(f"    - {'Std':<10}: {np.std(age_over_images):.2f}")
print(f"    - {'Min':<10}: {np.min(age_over_images):.2f}")
print(f"    - {'Q1':<10}: {np.percentile(age_over_images, q = 25, method = 'midpoint'):.2f}")
print(f"    - {'Median':<10}: {np.percentile(age_over_images, q = 50, method = 'midpoint'):.2f}")
print(f"    - {'Q3':<10}: {np.percentile(age_over_images, q = 75, method = 'midpoint'):.2f}")
print(f"    - {'Max':<10}: {np.max(age_over_images):.2f}")

# print wierd findings
print(f"Number of patients with age zero: {len(metadata_df[metadata_df['age'] == 0])}")
print(f"Number of patients with age below 18: {len(metadata_df[metadata_df['age'] < 18])}")

# age distribution over the patients
age_over_patients = metadata_df[['lesion_id', 'age']].groupby(['lesion_id']).agg('mean')['age'].to_numpy()
fig, ax = plt.subplots()
ax.hist(age_over_patients)
ax.set_title("Age Distribution over the Patients")
ax.set_ylabel("Frequency / Count")
ax.set_xlabel("Age")
plt.tight_layout()
plt.show()

print(f"Number of unique patient: {len(age_over_patients)}")
print(f"Number of N/A rows for age: {np.sum(np.isnan(age_over_patients))}")
age_over_patients = age_over_patients[~np.isnan(age_over_patients)]
print("Summary Statistics (ignore N/A values):")
print(f"    - {'Count':<10}: {len(age_over_patients)}")
print(f"    - {'Mean':<10}: {np.mean(age_over_patients):.2f}")
print(f"    - {'Std':<10}: {np.std(age_over_patients):.2f}")
print(f"    - {'Min':<10}: {np.min(age_over_patients):.2f}")
print(f"    - {'Q1':<10}: {np.percentile(age_over_patients, q = 25, method = 'midpoint'):.2f}")
print(f"    - {'Median':<10}: {np.percentile(age_over_patients, q = 50, method = 'midpoint'):.2f}")
print(f"    - {'Q3':<10}: {np.percentile(age_over_patients, q = 75, method = 'midpoint'):.2f}")
print(f"    - {'Max':<10}: {np.max(age_over_patients):.2f}")

"""### What is the sex distribution? Both over the images and over unique patients?"""

# gender information in the metadata
print(f"Number of N/A rows for gender: {len(metadata_df[metadata_df['sex'].isna()])}")

# gender distribution over the images
sex_over_images = metadata_df["sex"]
sexes, sex_counts = sex_over_images.value_counts().index.to_numpy(), sex_over_images.value_counts().values
fig, ax = plt.subplots()
ax.bar(sexes, sex_counts)
ax.set_title("Sex Distribution over the Images")
ax.set_ylabel("Frequency / Count")
ax.set_xlabel("Sex")

# add bar chart labels
for i, v in enumerate(sex_counts):
    plt.text(i, v, f"{v}\n({100 * v / np.sum(sex_counts):.2f}%)", ha='center', va='bottom')

ax.set_ylim(0, 6100)

plt.tight_layout()
plt.show()

# gender distribution over the images
sex_over_patient = metadata_df[['lesion_id', 'sex']].groupby(['lesion_id']).agg(lambda x: x.mode()[0])["sex"]
assert len(sex_over_patient) == len(metadata_df['lesion_id'].unique())
sexes, sex_counts = sex_over_patient.value_counts().index.to_numpy(), sex_over_patient.value_counts().values
fig, ax = plt.subplots()
ax.bar(sexes, sex_counts)
ax.set_title("Sex Distribution over the Images")
ax.set_ylabel("Frequency / Count")
ax.set_xlabel("Sex")

# add bar chart labels
for i, v in enumerate(sex_counts):
    plt.text(i, v, f"{v}\n({100 * v / np.sum(sex_counts):.2f}%)", ha='center', va='bottom')

ax.set_ylim(0, 4600)

plt.tight_layout()
plt.show()

"""### Is there a difference between genders when it comes to age (over the patients)?"""

# get the age and gender metadata
sex_over_patient = metadata_df[['lesion_id', 'sex']].groupby(['lesion_id']).agg(lambda x: x.mode()[0])
age_over_patients = metadata_df[['lesion_id', 'age']].groupby(['lesion_id']).agg('mean')['age']
sex_age_over_patients = pd.merge(age_over_patients, sex_over_patient, right_index = True, left_index = True)

# visualize sex vs age
male_patient_ages = sex_age_over_patients[sex_age_over_patients["sex"] == "male"]["age"]
female_patient_ages = sex_age_over_patients[sex_age_over_patients["sex"] == "female"]["age"]

fig, ax = plt.subplots()
ax.hist(female_patient_ages, alpha=0.5, label = "Female")
ax.hist(male_patient_ages, alpha=0.5, label = "Male")
ax.set_title("Age Distribution Segmented by Sex over the Patients")
ax.set_ylabel("Frequency / Count")
ax.set_xlabel("Age")
ax.legend()
plt.tight_layout()
plt.show()

print(f"Number of unique male patient: {len(male_patient_ages)}")
print(f"Number of N/A rows for male patient age: {np.sum(np.isnan(male_patient_ages))}")
male_patient_ages = male_patient_ages[~np.isnan(male_patient_ages)]
print("Summary Statistics (ignore N/A values) for male patient age:")
print(f"    - {'Count':<10}: {len(male_patient_ages)}")
print(f"    - {'Mean':<10}: {np.mean(male_patient_ages):.2f}")
print(f"    - {'Std':<10}: {np.std(male_patient_ages):.2f}")
print(f"    - {'Min':<10}: {np.min(male_patient_ages):.2f}")
print(f"    - {'Q1':<10}: {np.percentile(male_patient_ages, q = 25, method = 'midpoint'):.2f}")
print(f"    - {'Median':<10}: {np.percentile(male_patient_ages, q = 50, method = 'midpoint'):.2f}")
print(f"    - {'Q3':<10}: {np.percentile(male_patient_ages, q = 75, method = 'midpoint'):.2f}")
print(f"    - {'Max':<10}: {np.max(male_patient_ages):.2f}")

print("\n")

print(f"Number of unique female patient: {len(female_patient_ages)}")
print(f"Number of N/A rows for female patient age: {np.sum(np.isnan(female_patient_ages))}")
female_patient_ages = female_patient_ages[~np.isnan(female_patient_ages)]
print("Summary Statistics (ignore N/A values) for female patient age:")
print(f"    - {'Count':<10}: {len(female_patient_ages)}")
print(f"    - {'Mean':<10}: {np.mean(female_patient_ages):.2f}")
print(f"    - {'Std':<10}: {np.std(female_patient_ages):.2f}")
print(f"    - {'Min':<10}: {np.min(female_patient_ages):.2f}")
print(f"    - {'Q1':<10}: {np.percentile(female_patient_ages, q = 25, method = 'midpoint'):.2f}")
print(f"    - {'Median':<10}: {np.percentile(female_patient_ages, q = 50, method = 'midpoint'):.2f}")
print(f"    - {'Q3':<10}: {np.percentile(female_patient_ages, q = 75, method = 'midpoint'):.2f}")
print(f"    - {'Max':<10}: {np.max(female_patient_ages):.2f}")

"""#### Is the difference between male and female patient age statistically significantly?

We can use a statistical test to compare the means between the male and female groups and see if they are truly different. We will setup a null hypothesis that the mean age for males and mean age for females are the same and an alternative hypothesis where the mean age for males is higher than the mean age for females.
"""

# Perform independent t-test
from scipy import stats
t_statistic, p_value = stats.ttest_ind(male_patient_ages, female_patient_ages)

# Print the t-statistic and p-value
print(f"T-Statistic: {t_statistic}")
print(f"P-Value: {p_value}")

if p_value < 0.05:
    print(f"We reject the null hypothesis and can state that the mean age for male patients is higher than the mean age for female patients")
else:
    print(f"We fail to reject the null hypothesis and the mean age between male and female patients are not statistically significantly different")

"""### What is the distribution of diagnosis across the images?"""

# diagnosis distribution over the images
diagnosis_over_images = metadata_df["dx"]
diagoses, daignosis_counts = diagnosis_over_images.value_counts().index.to_numpy(), diagnosis_over_images.value_counts().values
fig, ax = plt.subplots()
ax.bar(diagoses, daignosis_counts)
ax.set_title("Diagnosis Distribution over the Images")
ax.set_ylabel("Frequency / Count")
ax.set_xlabel("Diagnosis")

# add bar chart labels
for i, v in enumerate(daignosis_counts):
    plt.text(i, v, f"{v}\n({100 * v / np.sum(daignosis_counts):.2f}%)", ha='center', va='bottom')

ax.set_ylim(0, 7500)

plt.tight_layout()
plt.show()

"""# Prepping the Data for Modeling"""

sample_image_path = "/content/images_1/ISIC_0024316.jpg"
sample_image = np.array(Image.open(sample_image_path))

# make a figure
fig, ax = plt.subplots()
plt.imshow(sample_image)

# Hide the axes
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
ax.spines['bottom'].set_visible(False)
ax.spines['left'].set_visible(False)

# Remove the ticks
ax.set_xticks([])
ax.set_yticks([])

plt.show()

print(f"Shape of our sample image: {sample_image.shape}")
print(f"The total number of pixels in an image: {np.prod(sample_image.shape)}")
print(f"Across our entire dataset of {len(os.listdir(image_data_folder_1)) + len(os.listdir(image_data_folder_2))} images, we have {np.prod(sample_image.shape)} pixels per image, resulting in {np.prod(sample_image.shape)*(len(os.listdir(image_data_folder_1)) + len(os.listdir(image_data_folder_2)))} numbers")

"""Key Takeways:
- We have high resolution images and training a model from scratch from these images will take time - we can expect that our ML models will probably take a while train
- One solution is to downsize each of the images such that it is more manageable to work with

Approaches:
- Build a (convolutional) neural network from scratch
- Use transfer learning (use an expert model) and then re-train our model
- We can focus on predicting the diagnosis or do binary classification on malignant vs benign

## Transform All Images and Cache Them

We will transform all of our 10015 images and then store in a folder.
"""

def image_transform(img, visualize = False):
    """
    Transform a (450, 600, 3) image to (150, 200) by:
        - turning to grayscale
        - downsizing the image
    """
    # grayscales my imahge
    img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)

    # resize my image
    img = cv2.resize(img, (200, 150))

    if visualize:
        # make a figure
        fig, ax = plt.subplots(figsize = (3, 4))
        plt.imshow(img, cmap = "gray", vmin = 0, vmax = 255)

        # Hide the axes
        ax.spines['top'].set_visible(False)
        ax.spines['right'].set_visible(False)
        ax.spines['bottom'].set_visible(False)
        ax.spines['left'].set_visible(False)

        # Remove the ticks
        ax.set_xticks([])
        ax.set_yticks([])

        plt.tight_layout()
        plt.show()

        print(f"My new image dimensions are: {img.shape}")

    return img

# gets my ISIC_IDs
ISIC_IDs = metadata_df["image_id"].values
assert len(ISIC_IDs) == 10015

for i, ISIC_ID in enumerate(ISIC_IDs):
    # load in the image
    try:
        image_path = f"/content/images_1/{ISIC_ID}.jpg"
        image = np.array(Image.open(image_path))
    except:
        image_path = f"/content/images_2/{ISIC_ID}.jpg"
        image = np.array(Image.open(image_path))

    # transform my image
    image = image_transform(image)
    assert image.shape == (150, 200)

    # save my image to a folder
    dirpath = "/content/image_grayscale_150_200"

    # check if the folder exists and creates if doesn't exist
    if not os.path.exists(dirpath):
        os.mkdir(dirpath)

    # save my file to the path
    filename = os.path.join(dirpath, f"{ISIC_ID}.jpg")
    cv2.imwrite(filename, image)

    # progress checker for every 500 images
    if i % 500 == 0:
        print(f"Progress: {100 * i / 10015:.2f}%")

print("COMPLETED!")

print(f"""
Before, our image was (450, 600, 3) which a total of {450 * 600 * 3} pixels per image.
That translates to 10015 * {450 * 600 * 3} =  {450 * 600 * 3 * 10015} total pixels in our dataset.

When we converted to grayscale and downsampled to the third image, our new dimensions are (150, 200)
That means we have {150 * 200} pixels per image, which {100* (150 * 200) / (450 * 600 * 3):.2f}% of the original image
That is a reduction in size of {100 - (100 * (150 * 200) / (450 * 600 * 3)):.2f}% from the original image

When we look across that total dataset, we know have {10015 * 150 * 200} total pixels.
This means we reduced the size of data by {100 - 100 * ((10015 * 150 * 200) / (10015 * 450 * 600 * 3)):.2f}%.
      """)

"""## Make Data Model Ready"""

# encode the labels as numbers
num_to_dx = {
    0: "akiec", 1: "bcc", 2: "bkl", 3: "df", 4: "mel", 5: "nv", 6: "vasc"
}
dx_to_num = {value: key for key, value in num_to_dx.items()}

# gathering images and labels
images = []
labels = []
for index, row in metadata_df.iterrows():
    # extract relevant columns
    ISIC_ID = row["image_id"]
    diagnosis = row["dx"]

    # get the image
    img_path = f"/content/image_grayscale_150_200/{ISIC_ID}.jpg"
    img = np.array(Image.open(img_path))
    assert img.shape == (150, 200)
    # plt.imshow(img)
    # plt.title(f"{ISIC_ID}: {diagnosis}")
    # plt.show()
    # print(ISIC_ID, diagnosis)

    # add the image and label to my list
    images.append(img)
    labels.append(dx_to_num[diagnosis])

    # progress checker for every 500 images
    if index % 500 == 0:
        print(f"Progress: {100 * index / 10015:.0f}%")

print("COMPLETED!")

assert len(labels) == 10015
assert len(images) == 10015

images = np.array(images)
print(f"Dimensions of images: {images.shape}")
labels = np.array(labels)
print(f"Dimensions of labels: {labels.shape}")

# 80 / 20 split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(images,
                                                    labels,
                                                    test_size = 0.2,
                                                    random_state = 10000)
# print shapes
print(f"X_train has shape: {X_train.shape}")
print(f"y_train has shape: {y_train.shape}")
print(f"X_test has shape: {X_test.shape}")
print(f"y_test has shape: {y_test.shape}")

"""# Models

## Model 1
"""

# simple 3 layer network
model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(150, 200)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(7)
])

# compile the model
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

# train the model with the train data
# normalize image to [0, 1] by dividing by 255
history = model.fit(X_train / 255, y_train, epochs=10, validation_data = (X_test / 255, y_test))

# summarize history for accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.ylim(0, 1)
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

probability_model = tf.keras.Sequential([model,
                                         tf.keras.layers.Softmax()])
predictions = probability_model.predict(X_test / 255)
y_pred = np.argmax(predictions, axis = 1)

# investigating the results using a confusion matrix (cm)
cm = confusion_matrix(y_test, y_pred)

# making a heatmap
fig, ax = plt.subplots(figsize=(10,10))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar = False,
            annot_kws={"fontsize":14},
            xticklabels=list(dx_to_num.keys()), yticklabels=list(dx_to_num.keys()))
ax.set_title("Confusion Matrix: 3 Layer Neural Network", fontsize = 24)
ax.set_ylabel("True Label", fontsize = 16)
ax.set_xlabel("Predicted Label", fontsize = 16)
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)

plt.show()

from sklearn import metrics
from sklearn.metrics import accuracy_score
print(f"Accuracy is what percent did the model get right overall?")
print(f"Accuracy: {accuracy_score(y_pred, y_test):.2f}\n")

print(metrics.classification_report(y_test, y_pred, digits=3))

"""## Model 2"""

# simple 3 layer network
model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 200, 1)), # first convolutional layer and taking in input
    tf.keras.layers.MaxPooling2D((2, 2)), # pooling refers to reducing the image size - it's type of data compression and filtering

    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'), # second convolutional layer
    tf.keras.layers.MaxPooling2D((2, 2)), # pooling refers to reducing the image size - it's type of data compression and filtering

    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(7)
])

# compile the model
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

# train the model with the train data
# normalize image to [0, 1] by dividing by 255
history = model.fit(X_train / 255, y_train, epochs=10, validation_data = (X_test / 255, y_test))

# summarize history for accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.ylim(0, 1)
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

probability_model = tf.keras.Sequential([model,
                                         tf.keras.layers.Softmax()])
predictions = probability_model.predict(X_test / 255)
y_pred = np.argmax(predictions, axis = 1)

# investigating the results using a confusion matrix (cm)
cm = confusion_matrix(y_test, y_pred)

# making a heatmap
fig, ax = plt.subplots(figsize=(10,10))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar = False,
            annot_kws={"fontsize":14},
            xticklabels=list(dx_to_num.keys()), yticklabels=list(dx_to_num.keys()))
ax.set_title("Confusion Matrix: 3 Layer Neural Network", fontsize = 24)
ax.set_ylabel("True Label", fontsize = 16)
ax.set_xlabel("Predicted Label", fontsize = 16)
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)

plt.show()

from sklearn import metrics
from sklearn.metrics import accuracy_score
print(f"Accuracy is what percent did the model get right overall?")
print(f"Accuracy: {accuracy_score(y_pred, y_test):.2f}\n")

print(metrics.classification_report(y_test, y_pred, digits=3))

"""## Model 3"""

# simple 3 layer network
model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(128, (5, 5), activation='relu', input_shape=(150, 200, 1)), # first convolutional layer and taking in input
    tf.keras.layers.MaxPooling2D((2, 2)), # pooling refers to reducing the image size - it's type of data compression and filtering
    tf.keras.layers.BatchNormalization(axis = 3),

    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'), # second convolutional layer
    tf.keras.layers.MaxPooling2D((2, 2)), # pooling refers to reducing the image size - it's type of data compression and filtering
    tf.keras.layers.BatchNormalization(axis = 3),

    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'), # second convolutional layer
    tf.keras.layers.MaxPooling2D((2, 2)), # pooling refers to reducing the image size - it's type of data compression and filtering
    tf.keras.layers.BatchNormalization(axis = 3),

    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(7)
])

# compile the model
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

# train the model with the train data
# normalize image to [0, 1] by dividing by 255
history = model.fit(X_train / 255, y_train, epochs=10, validation_data = (X_test / 255, y_test))

# summarize history for accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.ylim(0, 1)
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

probability_model = tf.keras.Sequential([model,
                                         tf.keras.layers.Softmax()])
predictions = probability_model.predict(X_test / 255)
y_pred = np.argmax(predictions, axis = 1)

# investigating the results using a confusion matrix (cm)
cm = confusion_matrix(y_test, y_pred)

# making a heatmap
fig, ax = plt.subplots(figsize=(10,10))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar = False,
            annot_kws={"fontsize":14},
            xticklabels=list(dx_to_num.keys()), yticklabels=list(dx_to_num.keys()))
ax.set_title("Confusion Matrix: 3 Layer Neural Network", fontsize = 24)
ax.set_ylabel("True Label", fontsize = 16)
ax.set_xlabel("Predicted Label", fontsize = 16)
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)

plt.show()

from sklearn import metrics
from sklearn.metrics import accuracy_score
print(f"Accuracy is what percent did the model get right overall?")
print(f"Accuracy: {accuracy_score(y_pred, y_test):.2f}\n")

print(metrics.classification_report(y_test, y_pred, digits=3))

"""## Model 4: Transfer Learning"""

# convert images to (150, 200, 3)
def image_transform(img, visualize = False):
    """
    Transform a (450, 600, 3) image to (150, 200, 3) by:
        - turning to grayscale (DO NOT)
        - downsizing the image
    """
    # grayscales my imahge
    # img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)

    # resize my image
    img = cv2.resize(img, (200, 150))

    if visualize:
        # make a figure
        fig, ax = plt.subplots(figsize = (3, 4))
        plt.imshow(img, cmap = "gray", vmin = 0, vmax = 255)

        # Hide the axes
        ax.spines['top'].set_visible(False)
        ax.spines['right'].set_visible(False)
        ax.spines['bottom'].set_visible(False)
        ax.spines['left'].set_visible(False)

        # Remove the ticks
        ax.set_xticks([])
        ax.set_yticks([])

        plt.tight_layout()
        plt.show()

        print(f"My new image dimensions are: {img.shape}")

    return img

# gets my ISIC_IDs
ISIC_IDs = metadata_df["image_id"].values
assert len(ISIC_IDs) == 10015

for i, ISIC_ID in enumerate(ISIC_IDs):
    # load in the image
    try:
        image_path = f"/content/images_1/{ISIC_ID}.jpg"
        image = np.array(Image.open(image_path))
    except:
        image_path = f"/content/images_2/{ISIC_ID}.jpg"
        image = np.array(Image.open(image_path))

    # transform my image
    image = image_transform(image)
    assert image.shape == (150, 200, 3)

    # save my image to a folder
    dirpath = "/content/image_color_150_200"

    # check if the folder exists and creates if doesn't exist
    if not os.path.exists(dirpath):
        os.mkdir(dirpath)

    # save my file to the path
    filename = os.path.join(dirpath, f"{ISIC_ID}.jpg")
    cv2.imwrite(filename, image)

    # progress checker for every 500 images
    if i % 500 == 0:
        print(f"Progress: {100 * i / 10015:.2f}%")

print("COMPLETED!")

# encode the labels as numbers
num_to_dx = {
    0: "akiec", 1: "bcc", 2: "bkl", 3: "df", 4: "mel", 5: "nv", 6: "vasc"
}
dx_to_num = {value: key for key, value in num_to_dx.items()}

# gathering images and labels
images = []
labels = []
for index, row in metadata_df.iterrows():
    # extract relevant columns
    ISIC_ID = row["image_id"]
    diagnosis = row["dx"]

    # get the image
    img_path = f"/content/image_color_150_200/{ISIC_ID}.jpg"
    img = np.array(Image.open(img_path))
    assert img.shape == (150, 200, 3)
    # plt.imshow(img)
    # plt.title(f"{ISIC_ID}: {diagnosis}")
    # plt.show()
    # print(ISIC_ID, diagnosis)

    # add the image and label to my list
    images.append(img)
    labels.append(dx_to_num[diagnosis])

    # progress checker for every 500 images
    if index % 500 == 0:
        print(f"Progress: {100 * index / 10015:.0f}%")

print("COMPLETED!")

assert len(labels) == 10015
assert len(images) == 10015

images = np.array(images)
print(f"Dimensions of images: {images.shape}")
labels = np.array(labels)
print(f"Dimensions of labels: {labels.shape}")

# 80 / 20 split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(images,
                                                    labels,
                                                    test_size = 0.2,
                                                    random_state = 10000)
# print shapes
print(f"X_train has shape: {X_train.shape}")
print(f"y_train has shape: {y_train.shape}")
print(f"X_test has shape: {X_test.shape}")
print(f"y_test has shape: {y_test.shape}")

# Create base model
base_model = tf.keras.applications.resnet50.ResNet50(
    weights='imagenet',
    input_shape=(150, 200, 3),
    include_top=False)
# Freeze base model
base_model.trainable = False

# Create new model on top.
inputs = tf.keras.Input(shape=(150, 200, 3))
x = base_model(inputs, training=False)
x = tf.keras.layers.GlobalAveragePooling2D()(x)
x = tf.keras.layers.Dense(256)(x)
x = tf.keras.layers.Dense(7)(x)
model = tf.keras.Model(inputs, x)

# compile the model
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

# train the model with the train data
# normalize image to [0, 1] by dividing by 255
history = model.fit(X_train / 255, y_train, epochs=10, validation_data = (X_test / 255, y_test))

# summarize history for accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.ylim(0, 1)
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

probability_model = tf.keras.Sequential([model,
                                         tf.keras.layers.Softmax()])
predictions = probability_model.predict(X_test / 255)
y_pred = np.argmax(predictions, axis = 1)

# investigating the results using a confusion matrix (cm)
cm = confusion_matrix(y_test, y_pred)

# making a heatmap
fig, ax = plt.subplots(figsize=(10,10))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar = False,
            annot_kws={"fontsize":14},
            xticklabels=list(dx_to_num.keys()), yticklabels=list(dx_to_num.keys()))
ax.set_title("Confusion Matrix: 3 Layer Neural Network", fontsize = 24)
ax.set_ylabel("True Label", fontsize = 16)
ax.set_xlabel("Predicted Label", fontsize = 16)
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)

plt.show()

from sklearn import metrics
from sklearn.metrics import accuracy_score
print(f"Accuracy is what percent did the model get right overall?")
print(f"Accuracy: {accuracy_score(y_pred, y_test):.2f}\n")

print(metrics.classification_report(y_test, y_pred, digits=3))

"""# Model 5: Binary Classification"""

binary_labels = []
for l in labels:
    if l in [0, 1, 4]:
        binary_labels.append(1)
    else:
        binary_labels.append(0)
binary_labels = np.array(binary_labels)

# 80 / 20 split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(images,
                                                    binary_labels,
                                                    test_size = 0.2,
                                                    random_state = 10000)
# print shapes
print(f"X_train has shape: {X_train.shape}")
print(f"y_train has shape: {y_train.shape}")
print(f"X_test has shape: {X_test.shape}")
print(f"y_test has shape: {y_test.shape}")

# simple 3 layer network
model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 200, 1)), # first convolutional layer and taking in input
    tf.keras.layers.MaxPooling2D((2, 2)), # pooling refers to reducing the image size - it's type of data compression and filtering

    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'), # second convolutional layer
    tf.keras.layers.MaxPooling2D((2, 2)), # pooling refers to reducing the image size - it's type of data compression and filtering

    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(7)
])

# compile the model
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

# train the model with the train data
# normalize image to [0, 1] by dividing by 255
history = model.fit(X_train / 255, y_train, epochs=10, validation_data = (X_test / 255, y_test))

# summarize history for accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.ylim(0, 1)
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

probability_model = tf.keras.Sequential([model,
                                         tf.keras.layers.Softmax()])
predictions = probability_model.predict(X_test / 255)
y_pred = np.argmax(predictions, axis = 1)

# investigating the results using a confusion matrix (cm)
cm = confusion_matrix(y_test, y_pred)

# making a heatmap
fig, ax = plt.subplots()
sns.set(font_scale=3.0) # Adjust to fit
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar = False,
            annot_kws={"fontsize":14},
            xticklabels=["benign", "malignant"], yticklabels=["benign", "malignant"],
            # xticklabels=list(dx_to_num.keys()), yticklabels=list(dx_to_num.keys())
            )
# ax.set_title("Confusion Matrix: 3 Layer Neural Network", fontsize = 24)
ax.set_ylabel("True Label", fontsize = 16)
ax.set_xlabel("Predicted Label", fontsize = 16)
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)

plt.show()

